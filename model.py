{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" [code]","metadata":{"_uuid":"ce322833-bd50-4f44-9a30-3f058845dd56","_cell_guid":"9451620a-cec6-441f-ae57-97b9f6bd0170","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"be798584-9220-41cc-8eef-a40fef889d82","_cell_guid":"8ac41bbd-9a34-4a45-99a9-d74df45bfbac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-01-19T07:33:41.422796Z","iopub.execute_input":"2023-01-19T07:33:41.423295Z","iopub.status.idle":"2023-01-19T07:33:41.453698Z","shell.execute_reply.started":"2023-01-19T07:33:41.423161Z","shell.execute_reply":"2023-01-19T07:33:41.452829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"fb69d6f7-fdf4-4a91-87df-c5644a926733","_cell_guid":"3e97788f-0c71-4aca-86c9-0dfadfbd7ff4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-01-19T07:34:27.447678Z","iopub.execute_input":"2023-01-19T07:34:27.448097Z","iopub.status.idle":"2023-01-19T07:34:40.867235Z","shell.execute_reply.started":"2023-01-19T07:34:27.448063Z","shell.execute_reply":"2023-01-19T07:34:40.865916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nimport os\n\n\"\"\"Adversarially Learned Inference\nPage 14: CelebA model hyperparameters\nOptimizer Adam (α = 10−4, β1 = 0.5)\nBatch size 100 Epochs 123\nLeaky ReLU slope 0.02\nWeight, bias initialization Isotropic gaussian (µ = 0, σ = 0.01), Constant(0)\n\"\"\"\nbatch_size = 64\n\nz_dim = 512         # Noise dimension\nimage_size = 64     # 64 x 64\nc_dim = 3           # for rgb\n\n\ndef generator(input_z, input_txt=None, is_train=True, reuse=False, batch_size=batch_size):\n    \"\"\" G(z) or G(z, RNN(txt)) / output (64, 64, 3) \"\"\"\n    s = image_size\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n    gf_dim = 128\n\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init = tf.random_normal_initializer(1., 0.02)\n    lrelu = lambda x: tl.act.lrelu(x, 0.2)\n\n    with tf.variable_scope(\"generator\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_z, name='g_inputz')\n\n        if input_txt is not None:\n            net_txt = InputLayer(input_txt, name='g_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim,\n                    act=lrelu,\n                    W_init = w_init, b_init=None, name='g_reduce_text/dense')\n            net_in = ConcatLayer([net_in, net_txt], concat_dim=1, name='g_concat_z_txt')\n\n        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,\n                W_init=w_init, b_init=None, name='g_h0/dense')\n        net_h0 = BatchNormLayer(net_h0, #act=tf.identity,\n                is_train=is_train, gamma_init=gamma_init, name='g_h0/batch_norm')\n        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')\n\n        net = Conv2d(net_h0, gf_dim*2, (1, 1), (1, 1),\n                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h1_res/batch_norm')\n        net = Conv2d(net, gf_dim*2, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d2')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h1_res/batch_norm2')\n        net = Conv2d(net, gf_dim*8, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d3')\n        net = BatchNormLayer(net, # act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h1_res/batch_norm3')\n        net_h1 = ElementwiseLayer(layer=[net_h0, net], combine_fn=tf.add, name='g_h1_res/add')\n        net_h1.outputs = lrelu(net_h1.outputs)\n\n        net_h2 = DeConv2d(net_h1, gf_dim*4, (4, 4), out_size=(s8, s8), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h2/decon2d')\n        # net_h2 = UpSampling2dLayer(net_h1, size=[s8, s8], is_scale=False, method=1,\n        #         align_corners=False, name='g_h2/upsample2d')\n        # net_h2 = Conv2d(net_h2, gf_dim*4, (3, 3), (1, 1),\n        #         padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2,# act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h2/batch_norm')\n\n        net = Conv2d(net_h2, gf_dim, (1, 1), (1, 1),\n                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h3_res/batch_norm')\n        net = Conv2d(net, gf_dim, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d2')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h3_res/batch_norm2')\n        net = Conv2d(net, gf_dim*4, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d3')\n        net = BatchNormLayer(net, #act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h3_res/batch_norm3')\n        net_h3 = ElementwiseLayer(layer=[net_h2, net], combine_fn=tf.add, name='g_h3_res/add')\n        net_h3.outputs = lrelu(net_h3.outputs)\n\n        net_h4 = DeConv2d(net_h3, gf_dim*2, (4, 4), out_size=(s4, s4), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h4/decon2d')\n        # net_h4 = UpSampling2dLayer(net_h3, size=[s4, s4], is_scale=False, method=1,\n        #         align_corners=False, name='g_h4/upsample2d')\n        # net_h4 = Conv2d(net_h4, gf_dim*2, (3, 3), (1, 1),\n        #         padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h4/conv2d')\n        net_h4 = BatchNormLayer(net_h4, act=lrelu,#tf.nn.relu,#lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='g_h4/batch_norm')\n\n        net_h5 = DeConv2d(net_h4, gf_dim, (4, 4), out_size=(s2, s2), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h5/decon2d')\n        # net_h5 = UpSampling2dLayer(net_h4, size=[s2, s2], is_scale=False, method=1,\n        #         align_corners=False, name='g_h5/upsample2d')\n        # net_h5 = Conv2d(net_h5, gf_dim, (3, 3), (1, 1),\n        #         padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h5/conv2d')\n        net_h5 = BatchNormLayer(net_h5, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h5/batch_norm')\n\n        net_ho = DeConv2d(net_h5, c_dim, (4, 4), out_size=(s, s), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_ho/decon2d')\n        # net_ho = UpSampling2dLayer(net_h9, size=[s, s], is_scale=False, method=1,\n        #         align_corners=False, name='g_ho/upsample2d')\n        # net_ho = Conv2d(net_ho, c_dim, (3, 3), (1, 1),\n        #         padding='SAME', act=None, W_init=w_init, name='g_ho/conv2d')\n        logits = net_ho.outputs\n        net_ho.outputs = tf.nn.tanh(net_ho.outputs)\n    return net_ho, logits\n\ndef encoder_simple(input_images, input_txt=None, is_train=True, reuse=False):\n    \"\"\" E(x) input (64, 64, 3), output z \"\"\"\n    s = image_size\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n    # 32 16 8 4\n    w_init = tf.random_normal_initializer(stddev=0.01)\n    gamma_init = tf.random_normal_initializer(1., 0.01)\n    df_dim = 64\n    lrelu = lambda x: tl.act.lrelu(x, 0.2)\n\n    with tf.variable_scope(\"encoder\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_images, name='ig_inputz')\n        # print(net_in.outputs)\n        # exit()\n        net_h0 = Conv2d(net_in, df_dim, (2, 2), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h0/conv2d')\n        net_h0 = BatchNormLayer(net_h0, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h0/batchnorm')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (7, 7), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1/batchnorm')\n\n        net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h2/batchnorm')\n\n        net_h3 = Conv2d(net_h2, df_dim*4, (7, 7), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h3/batchnorm')\n\n        net_h4 = Conv2d(net_h3, df_dim*8, (4, 4), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h4/conv2d')\n        net_h4 = BatchNormLayer(net_h4, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h4/batchnorm')\n\n        if input_txt is not None:\n            net_txt = InputLayer(input_txt, name='ig_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   W_init=w_init, b_init=None, name='ig_reduce_txt/dense')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim1')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim2')\n            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='ig_txt/tile')\n            net_h4_concat = ConcatLayer([net_h4, net_txt], concat_dim=3, name='ig_txt/concat')\n            net_h4 = Conv2d(net_h4_concat, df_dim*8, (1, 1), (1, 1),\n                   padding='SAME', W_init=w_init, b_init=None, name='ig_txt/conv2d_2')\n            net_h4 = BatchNormLayer(net_h4, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   is_train=is_train, gamma_init=gamma_init, name='ig_txt/batch_norm_2')\n        # print(net_h4.outputs) # (100, 8, 8, 512)\n        # exit()\n        # net_ho = Conv2d(net_h4, df_dim*8, (1, 1), (1, 1), act=None,\n        #         padding='VALID', W_init=w_init, b_init=None, name='ig_ho/conv2d')  # DH\n        # print(net_h4.outputs) # (100, 1, 1, 512)\n        # exit()\n        net_ho = FlattenLayer(net_h4, name='ig_ho/flatten')\n        net_ho = DenseLayer(net_ho, n_units=z_dim, name='ig_ho/dense')\n        # print(net_ho.outputs)\n        # exit()\n        return net_ho\n\n# def encoder(input_images, is_train=True, reuse=False):\n#     \"\"\" E(x) input (64, 64, 3), output z \"\"\"\n#     w_init = tf.random_normal_initializer(stddev=0.02)\n#     gamma_init=tf.random_normal_initializer(1., 0.02)\n#     df_dim = 128#64\n#     with tf.variable_scope(\"encoder\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         # (nc) x 64 x 64\n#         net_in = InputLayer(input_images, name='ig_input/images')\n#         net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n#                 padding='SAME', W_init=w_init, name='p_h0/conv2d')\n#\n#         net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig_h1/conv2d')\n#         net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h1/batchnorm')\n#         net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig_h2/conv2d')\n#         net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h2/batchnorm')\n#         net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig_h3/conv2d')\n#         net_h3 = BatchNormLayer(net_h3, #act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h3/batchnorm')\n#\n#         net_h = Conv2d(net_h3, df_dim*2, (1, 1), (1, 1), act=None,\n#                 padding='VALID', W_init=w_init, b_init=None, name='ig_h3/conv2d2')\n#         net_h = BatchNormLayer(net_h, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h3/batchnorm2')\n#         net_h = Conv2d(net_h, df_dim*2, (3, 3), (1, 1), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig_h3/conv2d3')\n#         net_h = BatchNormLayer(net_h, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h3/batchnorm3')\n#         net_h = Conv2d(net_h, df_dim*8, (3, 3), (1, 1), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig_h3/conv2d4')\n#         net_h = BatchNormLayer(net_h, #act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig_h3/batchnorm4')\n#         net_h3 = ElementwiseLayer(layer=[net_h3, net_h], combine_fn=tf.add, name='ig_h3/add')\n#         net_h3.outputs = tl.act.lrelu(net_h3.outputs, 0.2)\n#\n#         net_h4 = Conv2d(net_h3, df_dim*2, (4, 4), (1, 1), padding='SAME',\n#                 W_init=w_init, name='ig_h4/conv2d_2')\n#         # print(net_h4.outputs)   # (100, 4, 4, 256)\n#         # exit()\n#         # 1 x 1 x 1\n#         net_h4 = FlattenLayer(net_h4, name='ig_h4/flatten')\n#         net_h4 = DenseLayer(net_h4, n_units=z_dim,\n#                 act=tf.identity, W_init=w_init, b_init = None, name='ig/h4/embed')\n#     return net_h4\n\ndef encoder_resnet(input_images, input_txt=None, is_train=True, reuse=False):\n    \"\"\" E(x) 64x64 --> z \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init=tf.random_normal_initializer(1., 0.02)\n    lrelu = lambda x: tl.act.lrelu(x, 0.2)\n\n    df_dim = 128#64\n    with tf.variable_scope(\"encoder\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        # (nc) x 64 x 64\n        net_in = InputLayer(input_images, name='ig_input/images')\n        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lrelu,\n                padding='SAME', W_init=w_init, name='p_h0/conv2d')\n        # print(net_h0.outputs) # (100, 32, 32, 128)\n        # exit()\n        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1/batchnorm')\n        net_h1 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h1/conv2d2')\n        net_h1 = BatchNormLayer(net_h1, #act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1/batchnorm2')\n        # print(net_h1.outputs) # (100, 8, 8, 512)\n        # exit()\n        net = Conv2d(net_h1, df_dim*1, (1, 1), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h1_res/conv2d')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1_res/batchnorm')\n        net = Conv2d(net, df_dim*1, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h1_res/conv2d2')\n        net = BatchNormLayer(net, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1_res/batchnorm2')\n        net = Conv2d(net, df_dim*4, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h1_res/conv2d3')\n        net = BatchNormLayer(net, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='ig_h1_res/batchnorm3')\n        net_h1 = ElementwiseLayer(layer=[net_h1, net], combine_fn=tf.add, name='ig_h1_res/add')\n        net_h1.outputs = lrelu(net_h1.outputs)\n\n        # print(net_h1.outputs) # (100, 8, 8, 512)\n        # exit()\n\n        # net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n        #         padding='SAME', W_init=w_init, b_init=None, name='ig_h2/conv2d')\n        # net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n        #         is_train=is_train, gamma_init=gamma_init, name='ig_h2/batchnorm')\n        # print(net_h2.outputs)\n        # exit()\n\n        net_h2 = Conv2d(net_h1, df_dim*8, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='ig_h2/batchnorm')\n        # print(net_h3.outputs)\n        # exit()\n\n        net = Conv2d(net_h2, df_dim*2, (1, 1), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='ig_h3_res/conv2d2')\n        net = BatchNormLayer(net, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='ig_h3_res/batchnorm2')\n        net = Conv2d(net, df_dim*2, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h3_res/conv2d3')\n        net = BatchNormLayer(net, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='ig_h3_res/batchnorm3')\n        net = Conv2d(net, df_dim*8, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='ig_h3_res/conv2d4')\n        net = BatchNormLayer(net, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='ig_h3_res/batchnorm4')\n        net_h3 = ElementwiseLayer(layer=[net_h2, net], combine_fn=tf.add, name='ig_h3_res/add')\n        net_h3.outputs = lrelu(net_h3.outputs)\n        # print(net_h3.outputs)\n        # exit()\n        if input_txt is not None:\n            net_txt = InputLayer(input_txt, name='ig_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   W_init=w_init, b_init=None, name='ig_reduce_txt/dense')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim1')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim2')\n            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='ig_txt/tile')\n            net_h3_concat = ConcatLayer([net_h3, net_txt], concat_dim=3, name='ig_txt/concat')\n            net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1),\n                   padding='SAME', W_init=w_init, b_init=None, name='ig_txt/conv2d_2')\n            net_h3 = BatchNormLayer(net_h3, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   is_train=is_train, gamma_init=gamma_init, name='ig_txt/batch_norm_2')\n\n        net_h3 = Conv2d(net_h3, df_dim*4, (4, 4), (1, 1), padding='SAME',\n                W_init=w_init, name='ig_h3/conv2d_2')\n        # print(net_h3.outputs)\n        # exit()\n        net_ho = FlattenLayer(net_h3, name='ig_ho/flatten')\n        net_ho = DenseLayer(net_ho, n_units=z_dim, act=tf.identity,\n                W_init=w_init, b_init = None, name='ig/ho/embed')\n    return net_ho\n\ndef discriminator_x(input_images, input_txt=None, is_train=True, reuse=False):\n    \"\"\" D(x) input (64, 64, 3) \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.01)\n    # w_init1 = tf.random_normal_initializer(stddev=0.01 * 2 * 2)\n    # w_init2 = tf.random_normal_initializer(stddev=0.01 * 7 * 7)\n    # w_init3 = tf.random_normal_initializer(stddev=0.01 * 5 * 5)\n    # w_init4 = tf.random_normal_initializer(stddev=0.01 * 7 * 7)\n    # w_init5 = tf.random_normal_initializer(stddev=0.01 * 4 * 4)\n    gamma_init=tf.random_normal_initializer(1., 0.01)\n\n    with tf.variable_scope(\"discriminator_x\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n\n        net_in = InputLayer(input_images, name='dx_input/images')\n        net_h0 = Conv2d(net_in, df_dim, (2, 2), (1, 1), act=lrelu,\n                padding='VALID', W_init=w_init, name='dx_h0/conv2d')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (7, 7), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='dx_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='dx_h1/batchnorm')\n\n        net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='dx_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='dx_h2/batchnorm')\n\n        net_h3 = Conv2d(net_h2, df_dim*4, (7, 7), (2, 2), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='dx_h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='dx_h3/batchnorm')\n\n        net_h4 = Conv2d(net_h3, df_dim*8, (4, 4), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='dx_h4/conv2d')\n        net_h4 = BatchNormLayer(net_h4, act=lrelu,\n                is_train=is_train, gamma_init=gamma_init, name='dx_h4/batchnorm')\n\n        if input_txt is not None:\n            net_txt = InputLayer(input_txt, name='dx_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   W_init=w_init, b_init=None, name='dx_reduce_txt/dense')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim1')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim2')\n            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n            net_h4_concat = ConcatLayer([net_h4, net_txt], concat_dim=3, name='dx_txt/concat')\n            net_h4 = Conv2d(net_h4_concat, df_dim*8, (1, 1), (1, 1),\n                   padding='SAME', W_init=w_init, b_init=None, name='dx_txt/conv2d_2')\n            net_h4 = BatchNormLayer(net_h4, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n                   is_train=is_train, gamma_init=gamma_init, name='dx_txt/batch_norm_2')\n\n        # print(net_h4.outputs, df_dim*8)\n        # exit()\n        net_ho = FlattenLayer(net_h4, name='dx_ho/flatten')\n        # net_ho = DenseLayer(net_ho, n_units=z_dim, act=tf.identity,\n        #         W_init = w_init, name='dx_ho/dense') # 512\n        # print(net_ho.outputs)\n        # exit()\n        return net_ho\n\n# def discriminator_x(input_images, input_txt=None, is_train=True, reuse=False): # cnn_encoder_resnet\n#     \"\"\" D(x) or D(x, RNN(txt)) / x=(64, 64, 3), output z \"\"\"\n#     # https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py  d_encode_image\n#     w_init = tf.random_normal_initializer(stddev=0.02)\n#     gamma_init=tf.random_normal_initializer(1., 0.02)\n#     lrelu = lambda x: tl.act.lrelu(x, 0.2)\n#\n#     df_dim = 64\n#     with tf.variable_scope(\"discriminator_x\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         # (nc) x 64 x 64\n#         net_in = InputLayer(input_images, name='dx_input/images')\n#\n#         # net_in = DropoutLayer(net_in, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_in')\n#         net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                 padding='SAME', W_init=w_init, name='dx_h0/conv2d')\n#\n#         # net_h0 = DropoutLayer(net_h0, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_h0')\n#         net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h1/conv2d')\n#         net_h1 = BatchNormLayer(net_h1, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h1/batchnorm')\n#         # net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_h1')\n#         net_h1 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h1/conv2d2')\n#         net_h1 = BatchNormLayer(net_h1, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h1/batchnorm2')\n#         # net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_h2')\n#         net_h1 = Conv2d(net_h1, df_dim*8, (4, 4), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h1/conv2d3')\n#         net_h1 = BatchNormLayer(net_h1, #act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h1/batchnorm3')\n#\n#         # net_h3 = DropoutLayer(net_h3, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_h3')\n#         net_h = Conv2d(net_h1, df_dim*2, (1, 1), (1, 1), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h_res/conv2d2')\n#         net_h = BatchNormLayer(net_h, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h_res/batchnorm2')\n#         # net_h = DropoutLayer(net_h, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_resh1')\n#         net_h = Conv2d(net_h, df_dim*2, (3, 3), (1, 1), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h_res/conv2d3')\n#         net_h = BatchNormLayer(net_h, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h_res/batchnorm3')\n#         # net_h = DropoutLayer(net_h, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_resh2')\n#         net_h = Conv2d(net_h, df_dim*8, (3, 3), (1, 1), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h_res/conv2d4')\n#         net_h = BatchNormLayer(net_h, #act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h_res/batchnorm4')\n#         net_h2 = ElementwiseLayer(layer=[net_h1, net_h], combine_fn=tf.add, name='dx_h_res/add')\n#         net_h2.outputs = tl.act.lrelu(net_h2.outputs, 0.2)\n#\n#         if input_txt is not None:\n#             net_txt = InputLayer(input_txt, name='dx_input_txt')\n#             net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    W_init=w_init, b_init=None, name='dx_reduce_txt/dense')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim1')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim2')\n#             net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n#             net_h2_concat = ConcatLayer([net_h2, net_txt], concat_dim=3, name='dx_txt/concat')\n#             net_h2 = Conv2d(net_h2_concat, df_dim*8, (1, 1), (1, 1),\n#                    padding='SAME', W_init=w_init, b_init=None, name='dx_txt/conv2d_2')\n#             net_h2 = BatchNormLayer(net_h2, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    is_train=is_train, gamma_init=gamma_init, name='dx_txt/batch_norm_2')\n#\n#         # net_h2 = DropoutLayer(net_h2, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_after_concat')\n#         net_ho = Conv2d(net_h2, df_dim*2, (4, 4), (1, 1), padding='SAME',\n#                 W_init=w_init, name='dx_ho/conv2d_2')\n#         # print(net_ho.outputs)   # (100, 4, 4, 128)\n#         # 1 x 1 x 1\n#         # net_ho = DropoutLayer(net_ho, keep=0.8, is_fix=True, is_train=is_train, name='dx_drop_ho')\n#         net_ho = FlattenLayer(net_ho, name='dx_ho/flatten')\n#         # exit()\n#         net_ho = DenseLayer(net_ho, n_units=z_dim, act=tf.identity,\n#                 W_init=w_init, b_init = None, name='dx/h4/embed')\n#     return net_ho\n\ndef discriminator_z(input_z, is_train=True, reuse=False):\n    \"\"\" D(z) input z \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.01)\n    lrelu = lambda x: tl.act.lrelu(x, 0.02)\n\n    with tf.variable_scope(\"discriminator_z\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_z, name='dz_input/z')\n        # net_in = ReshapeLayer(net_in, [-1, 1, 1, z_dim], name='dz_reshape')\n\n        net_in = DropoutLayer(net_in, keep=0.8, is_fix=True, is_train=is_train, name='dz_in/drop')\n            # net_h0 = Conv2d(net_in, 1024, (1, 1), (1, 1), act=lrelu,\n            #         padding='VALID', W_init=w_init, name='dz_h0/conv2d')\n        net_h0 = DenseLayer(net_in, n_units=1024, act=lrelu,\n                W_init=w_init, name='dz_h0/conv2d')\n\n        net_h0 = DropoutLayer(net_h0, keep=0.8, is_fix=True, is_train=is_train, name='dz_h0/drop')\n            # net_h1 = Conv2d(net_h0, 1024, (1, 1), (1, 1), act=lrelu,\n            #         padding='VALID', W_init=w_init, name='dz_h1/conv2d')\n        net_h1 = DenseLayer(net_h0, n_units=1024, act=lrelu,\n                W_init=w_init, name='dz_h1/conv2d')\n\n        # net_h1 = FlattenLayer(net_h1, name='dz_flatten')\n        # print(net_h1.outputs) # 1024\n        # exit()\n        return net_h1\n\ndef discriminator_combine_xz(x, z, is_train=True, reuse=False):\n    \"\"\" combine D(x) or D(x, RNN(txt)) with D(z), output real/fake \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.01)\n    lrelu = lambda x: tl.act.lrelu(x, 0.02)\n\n    with tf.variable_scope(\"discriminator_xz\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in_x = InputLayer(x, name='d_input/x')\n        net_in_z = InputLayer(z, name='d_input/z')\n        net_in = ConcatLayer([net_in_z, net_in_x], concat_dim=1, name='d/concat')\n        # print(net_in.outputs)\n        # exit()\n        # net_in = ExpandDimsLayer(net_in, 1 , name='d/expanddim1')\n        # net_in = ExpandDimsLayer(net_in, 1 , name='d/expanddim2')\n\n        net_in = DropoutLayer(net_in, keep=0.8, is_fix=True, is_train=is_train, name='d_in/drop')\n        # net_h0 = Conv2d(net_in, 2048, (1, 1), (1, 1), act=lrelu,\n        #         padding='VALID', W_init=w_init, name='d_h0/conv2d')\n        net_h0 = DenseLayer(net_in, n_units=1024,#2048,\n                act=lrelu,\n                W_init=w_init, name='d_h0/conv2d')\n\n        net_h0 = DropoutLayer(net_h0, keep=0.8, is_fix=True, is_train=is_train, name='d_h0/drop')\n        # net_h1 = Conv2d(net_h0, 2048, (1, 1), (1, 1), act=lrelu,\n        #         padding='VALID', W_init=w_init, name='d_h1/conv2d')\n        net_h1 = DenseLayer(net_h0, n_units=1024,#2048,\n                act=lrelu,\n                W_init=w_init, name='d_h1/conv2d')\n\n        net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, is_train=is_train, name='d_h1/drop')\n        # net_ho = Conv2d(net_h1, 1, (1, 1), (1, 1), act=None,\n        #         padding='VALID', W_init=w_init, name='d_ho/conv2d')\n        net_ho = DenseLayer(net_h1, n_units=1, act=tf.identity,\n                W_init=w_init, name='d_ho/conv2d')\n        # print(net_ho.outputs) # 1\n        # exit()\n        # net_ho = FlattenLayer(net_ho, name='d_ho/flatten')\n        # print(net_ho.outputs) # 1\n        # exit()\n        logits = net_ho.outputs\n        net_ho.outputs = tf.nn.sigmoid(net_ho.outputs)\n        return net_ho, logits\n\ndef discriminator(x, z, input_txt=None, is_train=True, reuse=False):\n    \"\"\" D(x, z) or D(x, z, text)\n    x=64x64\n    \"\"\"\n    net_z = discriminator_z(z, is_train=is_train, reuse=reuse)\n    net_x = discriminator_x(x, input_txt=input_txt, is_train=is_train, reuse=reuse)\n    net_d, logits = discriminator_combine_xz(net_x.outputs, net_z.outputs, is_train=is_train, reuse=reuse)\n    net_d.all_params.extend(net_x.all_params)\n    net_d.all_params.extend(net_z.all_params)\n    return net_d, logits\n\n## follow DCGAN architecture / WORK but no deep enough for flower dataset\n# def generator(input_z, input_txt=None, is_train=True, reuse=False, batch_size=batch_size):\n#     \"\"\" G(z) input z, output (64, 64, 3) \"\"\"\n#     s = image_size\n#     s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n#     # 32 16 8 4\n#     w_init = tf.random_normal_initializer(stddev=0.01)\n#     gamma_init = tf.random_normal_initializer(1., 0.01)\n#     gf_dim = 128\n#\n#     with tf.variable_scope(\"generator\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         net_in = InputLayer(input_z, name='g_inputz')\n#\n#         if input_txt is not None:\n#             net_txt = InputLayer(input_txt, name='g_input_txt')\n#             net_txt = DenseLayer(net_txt, n_units=t_dim,\n#                     act=lambda x: tl.act.lrelu(x, 0.2),\n#                     W_init = w_init, b_init=None, name='g_reduce_text/dense')\n#             # paper 4.1 : and then concatenated to the noise vector z\n#             net_in = ConcatLayer([net_in, net_txt], concat_dim=1, name='g_concat_z_txt')\n#\n#         net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,\n#                 W_init=w_init, b_init=None, name='g_h0/dense')\n#         net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')\n#         net_h0 = BatchNormLayer(net_h0, act=tf.nn.relu, is_train=is_train,\n#                 gamma_init=gamma_init, name='g_h0/batch_norm')\n#\n#         net_h1 = DeConv2d(net_h0, gf_dim*4, (5, 5), out_size=(s8, s8), strides=(2, 2),\n#                 padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h1/decon2d')\n#         net_h1 = BatchNormLayer(net_h1, act=tf.nn.relu, is_train=is_train,\n#                 gamma_init=gamma_init, name='g_h1/batch_norm')\n#\n#         net_h2 = DeConv2d(net_h1, gf_dim*2, (5, 5), out_size=(s4, s4), strides=(2, 2),\n#                 padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h2/decon2d')\n#         net_h2 = BatchNormLayer(net_h2, act=tf.nn.relu, is_train=is_train,\n#                 gamma_init=gamma_init, name='g_h2/batch_norm')\n#\n#         net_h3 = DeConv2d(net_h2, gf_dim, (5, 5), out_size=(s2, s2), strides=(2, 2),\n#                 padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=None, name='g_h3/decon2d')\n#         net_h3 = BatchNormLayer(net_h3, act=tf.nn.relu, is_train=is_train,\n#                 gamma_init=gamma_init, name='g_h3/batch_norm')\n#\n#         net_h4 = DeConv2d(net_h3, c_dim, (5, 5), out_size=(s, s), strides=(2, 2),\n#                 padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h4/decon2d')\n#         logits = net_h4.outputs\n#         net_h4.outputs = tf.nn.tanh(net_h4.outputs)\n#     return net_h4, logits\n#\n# def encoder(input_images, input_txt=None, is_train=True, reuse=False):\n#     \"\"\" E(x) input (64, 64, 3), output z \"\"\"\n#     s = image_size\n#     s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n#     # 32 16 8 4\n#     w_init = tf.random_normal_initializer(stddev=0.01)\n#     gamma_init = tf.random_normal_initializer(1., 0.01)\n#     df_dim = 128\n#\n#     with tf.variable_scope(\"encoder\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         net_in = InputLayer(input_images, name='ig_inputz')\n#\n#         net_h0 = Conv2d(net_in, df_dim, (5, 5), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n#                 padding='SAME', W_init=w_init, name='ig/h0/conv2d')\n#\n#         net_h1 = Conv2d(net_h0, df_dim*2, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig/h1/conv2d')\n#         net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig/h1/batch_norm')\n#\n#         # if name != 'cnn': # debug for training image encoder in step 2\n#         #     net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, name='p/h1/drop')\n#\n#         net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig/h2/conv2d')\n#         net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig/h2/batch_norm')\n#\n#         # if name != 'cnn': # debug for training image encoder in step 2\n#         #     net_h2 = DropoutLayer(net_h2, keep=0.8, is_fix=True, name='p/h2/drop')\n#\n#         net_h3 = Conv2d(net_h2, df_dim*8, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='ig/h3/conv2d')\n#         net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='ig/h3/batch_norm')\n#\n#         # if name != 'cnn': # debug for training image encoder in step 2\n#         #     net_h3 = DropoutLayer(net_h3, keep=0.8, is_fix=True, name='p/h3/drop')\n#         # print(net_h3.outputs)\n#         # exit()\n#\n#         if input_txt is not None:\n#             net_txt = InputLayer(input_txt, name='ig_input_txt')\n#             net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    W_init=w_init, b_init=None, name='ig_reduce_txt/dense')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim1')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='ig_txt/expanddim2')\n#             net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='ig_txt/tile')\n#             net_h3_concat = ConcatLayer([net_h3, net_txt], concat_dim=3, name='ig_txt/concat')\n#             net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1),\n#                    padding='SAME', W_init=w_init, b_init=None, name='ig_txt/conv2d_2')\n#             net_h3 = BatchNormLayer(net_h3, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    is_train=is_train, gamma_init=gamma_init, name='ig_txt/batch_norm_2')\n#\n#         net_h4 = FlattenLayer(net_h3, name='ig/h4/flatten')\n#         net_h4 = DenseLayer(net_h4, n_units=z_dim, act=tf.identity,\n#                 W_init = w_init, b_init = None, name='ig/h4/embed')\n#\n#         ## DH add\n#         # print(\"WARNING: FORCE ENCODER OUTPUT GAUSSIAN DISTRIBUTION !\")\n#         # mean, var = tf.nn.moments(net_h4.outputs, axes=[1])\n#         # mean = tf.expand_dims(mean, 1)\n#         # var = tf.expand_dims(var, 1)\n#         # net_h4.outputs = (net_h4.outputs - mean) / tf.sqrt(var)\n#     return net_h4\n#\n# def discriminator_x(input_images, input_txt=None, is_train=True, reuse=False):\n#     \"\"\" D(x) input (64, 64, 3) \"\"\"\n#     w_init = tf.random_normal_initializer(stddev=0.01)\n#     gamma_init=tf.random_normal_initializer(1., 0.01)\n#     df_dim = 64\n#\n#     with tf.variable_scope(\"discriminator_x\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#\n#         net_in = InputLayer(input_images, name='dx_input/images')\n#         net_h0 = Conv2d(net_in, df_dim, (5, 5), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n#                 padding='SAME', W_init=w_init, name='dx_h0/conv2d')  # (64, 32, 32, 64)\n#\n#         net_h1 = Conv2d(net_h0, df_dim*2, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h1/conv2d')\n#         net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h1/batchnorm') # (64, 16, 16, 128)\n#\n#         net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h2/conv2d')\n#         net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h2/batchnorm')    # (64, 8, 8, 256)\n#\n#         net_h3 = Conv2d(net_h2, df_dim*8, (5, 5), (2, 2), act=None,\n#                 padding='SAME', W_init=w_init, b_init=None, name='dx_h3/conv2d')\n#         net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n#                 is_train=is_train, gamma_init=gamma_init, name='dx_h3/batchnorm') # (64, 4, 4, 512)  paper 4.1: when the spatial dim of the D is 4x4, we replicate the description embedding spatially and perform a depth concatenation\n#\n#         if input_txt is not None:\n#             net_txt = InputLayer(input_txt, name='dx_input_txt')\n#             net_txt = DenseLayer(net_txt, n_units=t_dim, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    W_init=w_init, b_init=None, name='dx_reduce_txt/dense')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim1')\n#             net_txt = ExpandDimsLayer(net_txt, 1, name='dx_txt/expanddim2')\n#             net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n#             net_h3_concat = ConcatLayer([net_h3, net_txt], concat_dim=3, name='dx_txt/concat')\n#             net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1),\n#                    padding='SAME', W_init=w_init, b_init=None, name='dx_txt/conv2d_2')\n#             net_h3 = BatchNormLayer(net_h3, act=lrelu,#lambda x: tl.act.lrelu(x, 0.2),\n#                    is_train=is_train, gamma_init=gamma_init, name='dx_txt/batch_norm_2')\n#\n#         net_h4 = FlattenLayer(net_h3, name='dx_h4/flatten')          # (64, 8192)\n#\n#         net_h4 = DenseLayer(net_h4, n_units=512, act=tf.identity,\n#                 W_init = w_init, name='dx_h4/dense')\n#         # print(net_h4.outputs)\n#         # exit()\n#         # net_h4 = DenseLayer(net_h4, n_units=1, act=tf.identity,\n#         #         W_init = w_init, name='d_h4/dense')\n#         # logits = net_h4.outputs\n#         # net_h4.outputs = tf.nn.sigmoid(net_h4.outputs)  # (64, 1)\n#     return net_h4\n#\n# def discriminator_z(input_z, is_train=True, reuse=False):\n#     \"\"\" D(z) input z \"\"\"\n#     w_init = tf.random_normal_initializer(stddev=0.01)\n#     lrelu = lambda x: tl.act.lrelu(x, 0.2)\n#     with tf.variable_scope(\"discriminator_z\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         net_in = InputLayer(input_z, name='dz_input/z')\n#         # net_in = ReshapeLayer(net_in, [-1, 1, 1, z_dim], name='dz_reshape')\n#\n#         # if is_train:\n#         #     net_in = DropoutLayer(net_in, keep=0.8, is_fix=True, name='dz_in/drop')\n#         # net_h0 = Conv2d(net_in, 1024, (1, 1), (1, 1), act=lrelu,\n#         #         padding='VALID', W_init=w_init, name='dz_h0/conv2d')\n#         net_h0 = DenseLayer(net_in, n_units=512, act=lrelu, W_init=w_init, name='dz_h0/conv2d')\n#\n#         # if is_train:\n#         #     net_h0 = DropoutLayer(net_h0, keep=0.8, is_fix=True, name='dz_h0/drop')\n#         # net_h1 = Conv2d(net_h0, 1024, (1, 1), (1, 1), act=lrelu,\n#         #         padding='VALID', W_init=w_init, name='dz_h1/conv2d')\n#         net_h1 = DenseLayer(net_h0, n_units=512, act=lrelu, W_init=w_init, name='dz_h1/conv2d')\n#\n#         # net_h1 = FlattenLayer(net_h1, name='dz_flatten')\n#         # print(net_h1.outputs) # 512\n#         # exit()\n#         return net_h1\n#\n# def discriminator_combine_xz(x, z, is_train=True, reuse=False):\n#     \"\"\" input D(x), D(z), output real/fake \"\"\"\n#     w_init = tf.random_normal_initializer(stddev=0.01)\n#     lrelu = lambda x: tl.act.lrelu(x, 0.2)\n#\n#     with tf.variable_scope(\"discriminator\", reuse=reuse):\n#         tl.layers.set_name_reuse(reuse)\n#         net_in_x = InputLayer(x, name='d_input/x')\n#         net_in_z = InputLayer(z, name='d_input/z')\n#         net_in = ConcatLayer([net_in_z, net_in_x], concat_dim=1, name='d/concat')\n#         # print(net_in.outputs)\n#         # exit()\n#         # net_in = ExpandDimsLayer(net_in, 1 , name='d/expanddim1')\n#         # net_in = ExpandDimsLayer(net_in, 1 , name='d/expanddim2')\n#\n#         # if is_train:\n#         #     net_in = DropoutLayer(net_in, keep=0.8, is_fix=True, name='d_in/drop')\n#         # net_h0 = Conv2d(net_in, 2048, (1, 1), (1, 1), act=lrelu,\n#         #         padding='VALID', W_init=w_init, name='d_h0/conv2d')\n#         net_h0 = DenseLayer(net_in, n_units=1024, act=lrelu, W_init=w_init, name='d_h0/conv2d')\n#\n#         # if is_train:\n#         #     net_h0 = DropoutLayer(net_h0, keep=0.8, is_fix=True, name='d_h0/drop')\n#         # net_h1 = Conv2d(net_h0, 2048, (1, 1), (1, 1), act=lrelu,\n#         #         padding='VALID', W_init=w_init, name='d_h1/conv2d')\n#         net_h1 = DenseLayer(net_h0, n_units=1024, act=lrelu, W_init=w_init, name='d_h1/conv2d')\n#\n#         # if is_train:\n#         #     net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, name='d_h1/drop')\n#         # net_ho = Conv2d(net_h1, 1, (1, 1), (1, 1), act=None,\n#         #         padding='VALID', W_init=w_init, name='d_ho/conv2d')\n#         net_ho = DenseLayer(net_h1, n_units=1, act=tf.identity,#lrelu,\n#                 W_init=w_init, name='d_ho/conv2d')\n#         # print(net_ho.outputs) # 1\n#         # exit()\n#         # net_ho = FlattenLayer(net_ho, name='d_ho/flatten')\n#         # print(net_ho.outputs) # 1\n#         # exit()\n#         logits = net_ho.outputs\n#         net_ho.outputs = tf.nn.sigmoid(net_ho.outputs)\n#         return net_ho, logits\n#\n# def discriminator(x, z, is_train=True, reuse=False):\n#     \"\"\" D(x, z) \"\"\"\n#     net_z = discriminator_z(z, is_train=is_train, reuse=reuse)\n#     net_x = discriminator_x(x, is_train=is_train, reuse=reuse)\n#     net_d, logits = discriminator_combine_xz(net_x.outputs, net_z.outputs, is_train=is_train, reuse=reuse)\n#     net_d.all_params.extend(net_x.all_params)\n#     net_d.all_params.extend(net_z.all_params)\n#     return net_d, logits\n\n\n\n## for text-to-image mapping ===================================================\nt_dim = 128         # text feature dimension\nrnn_hidden_size = t_dim\nvocab_size = 8000\nword_embedding_size = 256\nkeep_prob = 1.0\n\ndef rnn_embed(input_seqs, is_train=True, reuse=False, return_embed=False):\n    \"\"\" txt --> t_dim \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    if tf.__version__ <= '0.12.1':\n        LSTMCell = tf.nn.rnn_cell.LSTMCell\n    else:\n        LSTMCell = tf.contrib.rnn.BasicLSTMCell\n    with tf.variable_scope(\"rnnftxt\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        network = EmbeddingInputlayer(\n                     inputs = input_seqs,\n                     vocabulary_size = vocab_size,\n                     embedding_size = word_embedding_size,\n                     E_init = w_init,\n                     name = 'rnn/wordembed')\n        network = DynamicRNNLayer(network,\n                     cell_fn = LSTMCell,\n                     cell_init_args = {'state_is_tuple' : True, 'reuse': reuse},  # for TF1.1, TF1.2 dont need to set reuse\n                     n_hidden = rnn_hidden_size,\n                     dropout = (keep_prob if is_train else None),\n                     initializer = w_init,\n                     sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),\n                     return_last = True,\n                     name = 'rnn/dynamic')\n        return network\n\ndef cnn_encoder(inputs, is_train=True, reuse=False, name='cnnftxt', return_h3=False):\n    \"\"\" 64x64 --> t_dim, for text-image mapping \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init = tf.random_normal_initializer(1., 0.02)\n    df_dim = 64\n\n    with tf.variable_scope(name, reuse=reuse):\n        tl.layers.set_name_reuse(True)\n\n        net_in = InputLayer(inputs, name='/in')\n        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n                padding='SAME', W_init=w_init, name='cnnf/h0/conv2d')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='cnnf/h1/batch_norm')\n\n        # if name != 'cnn': # debug for training image encoder in step 2\n        #     net_h1 = DropoutLayer(net_h1, keep=0.8, is_fix=True, name='p/h1/drop')\n\n        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='cnnf/h2/batch_norm')\n\n        # if name != 'cnn': # debug for training image encoder in step 2\n        #     net_h2 = DropoutLayer(net_h2, keep=0.8, is_fix=True, name='p/h2/drop')\n\n        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='cnnf/h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='cnnf/h3/batch_norm')\n\n        # if name != 'cnn': # debug for training image encoder in step 2\n        #     net_h3 = DropoutLayer(net_h3, keep=0.8, is_fix=True, name='p/h3/drop')\n\n        net_h4 = FlattenLayer(net_h3, name='cnnf/h4/flatten')\n        net_h4 = DenseLayer(net_h4, n_units= (z_dim if name == 'z_encoder' else t_dim),\n                act=tf.identity,\n                W_init = w_init, b_init = None, name='cnnf/h4/embed')\n    if return_h3:\n        return net_h4, net_h3\n    else:\n        return net_h4\n\n\n## simple g1, d1 ===============================================================\ndef generator_txt2img_simple(input_z, input_rnn_embed=None, is_train=True, reuse=False, batch_size=64):\n    \"\"\" z + (txt) --> 64x64 \"\"\"\n    s = image_size\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    b_init = None # tf.constant_initializer(value=0.0)\n    gamma_init = tf.random_normal_initializer(1., 0.02)\n    gf_dim = 128\n\n    with tf.variable_scope(\"generator\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_z, name='g_inputz')\n\n        if input_rnn_embed is not None:\n            net_txt = InputLayer(input_rnn_embed, name='g_rnn_embed_input')\n            net_txt = DenseLayer(net_txt, n_units=t_dim,\n                    act=lambda x: tl.act.lrelu(x, 0.2),\n                    W_init = w_init, b_init=None, name='g_reduce_text/dense')\n            net_in = ConcatLayer([net_in, net_txt], concat_dim=1, name='g_concat_z_seq')\n        else:\n            print(\"No text info will be used, i.e. normal DCGAN\")\n\n        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,\n                W_init=w_init, b_init=b_init, name='g_h0/dense')\n        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')\n        net_h0 = BatchNormLayer(net_h0, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h0/batch_norm')\n\n        net_h1 = DeConv2d(net_h0, gf_dim*4, (4, 4), out_size=(s8, s8), strides=(2, 2), # stackGI use (4, 4) https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h1/decon2d')\n        net_h1 = BatchNormLayer(net_h1, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h1/batch_norm')\n\n        net_h2 = DeConv2d(net_h1, gf_dim*2, (4, 4), out_size=(s4, s4), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h2/decon2d')\n        net_h2 = BatchNormLayer(net_h2, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h2/batch_norm')\n\n        net_h3 = DeConv2d(net_h2, gf_dim, (4, 4), out_size=(s2, s2), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h3/decon2d')\n        net_h3 = BatchNormLayer(net_h3, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h3/batch_norm')\n\n        net_h4 = DeConv2d(net_h3, c_dim, (4, 4), out_size=(s, s), strides=(2, 2),\n                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h4/decon2d')\n        logits = net_h4.outputs\n        net_h4.outputs = tf.nn.tanh(net_h4.outputs)\n    return net_h4, logits\n\ndef discriminator_txt2img_simple(input_images, input_rnn_embed=None, is_train=True, reuse=False):\n    \"\"\" 64x64 + (txt) --> real/fake \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    b_init = None # tf.constant_initializer(value=0.0)\n    gamma_init=tf.random_normal_initializer(1., 0.02)\n    df_dim = 64\n\n    with tf.variable_scope(\"discriminator\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n\n        net_in = InputLayer(input_images, name='d_input/images')\n        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n                padding='SAME', W_init=w_init, name='d_h0/conv2d')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=b_init, name='d_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm')\n\n        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=b_init, name='d_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')\n\n        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=b_init, name='d_h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm')\n\n        if input_rnn_embed is not None:\n            net_txt = InputLayer(input_rnn_embed, name='d_rnn_embed_input')\n            net_txt = DenseLayer(net_txt, n_units=t_dim,\n                   act=lambda x: tl.act.lrelu(x, 0.2),\n                   W_init=w_init, b_init=None, name='d_reduce_txt/dense')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim1')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim2')\n            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n            net_h3_concat = ConcatLayer([net_h3, net_txt], concat_dim=3, name='d_h3_concat')\n            # net_h3_concat = net_h3 # no text info\n            net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1),\n                   padding='SAME', W_init=w_init, b_init=b_init, name='d_h3/conv2d_2')\n            net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n                   is_train=is_train, gamma_init=gamma_init, name='d_h3/batch_norm_2')\n        else:\n            print(\"No text info will be used, i.e. normal DCGAN\")\n\n        net_h4 = FlattenLayer(net_h3, name='d_h4/flatten')\n        net_h4 = DenseLayer(net_h4, n_units=1, act=tf.identity,\n                W_init = w_init, name='d_h4/dense')\n        logits = net_h4.outputs\n        net_h4.outputs = tf.nn.sigmoid(net_h4.outputs)\n    return net_h4, logits\n\n\n## default g1, d1 ==============================================================\ndef generator_txt2img_resnet(input_z, t_txt=None, is_train=True, reuse=False, batch_size=batch_size):\n    \"\"\" z + (txt) --> 64x64 \"\"\"\n    # https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py\n    s = image_size # output image size [64]\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n    gf_dim = 128\n\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init = tf.random_normal_initializer(1., 0.02)\n\n    with tf.variable_scope(\"generator\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_z, name='g_inputz')\n\n        if t_txt is not None:\n            net_txt = InputLayer(t_txt, name='g_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim,\n                act=lambda x: tl.act.lrelu(x, 0.2), W_init=w_init, name='g_reduce_text/dense')\n            net_in = ConcatLayer([net_in, net_txt], concat_dim=1, name='g_concat_z_txt')\n\n        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,\n                W_init=w_init, b_init=None, name='g_h0/dense')\n        net_h0 = BatchNormLayer(net_h0,  #act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h0/batch_norm')\n        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')\n\n        net = Conv2d(net_h0, gf_dim*2, (1, 1), (1, 1),\n                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d')\n        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h1_res/batch_norm')\n        net = Conv2d(net, gf_dim*2, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d2')\n        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h1_res/batch_norm2')\n        net = Conv2d(net, gf_dim*8, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h1_res/conv2d3')\n        net = BatchNormLayer(net, # act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h1_res/batch_norm3')\n        net_h1 = ElementwiseLayer(layer=[net_h0, net], combine_fn=tf.add, name='g_h1_res/add')\n        net_h1.outputs = tf.nn.relu(net_h1.outputs)\n\n        # Note: you can also use DeConv2d to replace UpSampling2dLayer and Conv2d\n        # net_h2 = DeConv2d(net_h1, gf_dim*4, (4, 4), out_size=(s8, s8), strides=(2, 2),\n        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h2/decon2d')\n        net_h2 = UpSampling2dLayer(net_h1, size=[s8, s8], is_scale=False, method=1,\n                align_corners=False, name='g_h2/upsample2d')\n        net_h2 = Conv2d(net_h2, gf_dim*4, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2,# act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h2/batch_norm')\n\n        net = Conv2d(net_h2, gf_dim, (1, 1), (1, 1),\n                padding='VALID', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d')\n        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h3_res/batch_norm')\n        net = Conv2d(net, gf_dim, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d2')\n        net = BatchNormLayer(net, act=tf.nn.relu, is_train=is_train,\n                gamma_init=gamma_init, name='g_h3_res/batch_norm2')\n        net = Conv2d(net, gf_dim*4, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h3_res/conv2d3')\n        net = BatchNormLayer(net, #act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h3_res/batch_norm3')\n        net_h3 = ElementwiseLayer(layer=[net_h2, net], combine_fn=tf.add, name='g_h3/add')\n        net_h3.outputs = tf.nn.relu(net_h3.outputs)\n\n        # net_h4 = DeConv2d(net_h3, gf_dim*2, (4, 4), out_size=(s4, s4), strides=(2, 2),\n        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h4/decon2d'),\n        net_h4 = UpSampling2dLayer(net_h3, size=[s4, s4], is_scale=False, method=1,\n                align_corners=False, name='g_h4/upsample2d')\n        net_h4 = Conv2d(net_h4, gf_dim*2, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h4/conv2d')\n        net_h4 = BatchNormLayer(net_h4, act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h4/batch_norm')\n\n        # net_h5 = DeConv2d(net_h4, gf_dim, (4, 4), out_size=(s2, s2), strides=(2, 2),\n        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, b_init=b_init, name='g_h5/decon2d')\n        net_h5 = UpSampling2dLayer(net_h4, size=[s2, s2], is_scale=False, method=1,\n                align_corners=False, name='g_h5/upsample2d')\n        net_h5 = Conv2d(net_h5, gf_dim, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, b_init=None, name='g_h5/conv2d')\n        net_h5 = BatchNormLayer(net_h5, act=tf.nn.relu,\n                is_train=is_train, gamma_init=gamma_init, name='g_h5/batch_norm')\n\n        # net_ho = DeConv2d(net_h5, c_dim, (4, 4), out_size=(s, s), strides=(2, 2),\n        #         padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_ho/decon2d')\n        net_ho = UpSampling2dLayer(net_h5, size=[s, s], is_scale=False, method=1,\n                align_corners=False, name='g_ho/upsample2d')\n        net_ho = Conv2d(net_ho, c_dim, (3, 3), (1, 1),\n                padding='SAME', act=None, W_init=w_init, name='g_ho/conv2d')\n        logits = net_ho.outputs\n        net_ho.outputs = tf.nn.tanh(net_ho.outputs)\n    return net_ho, logits\n\ndef discriminator_txt2img_resnet(input_images, t_txt=None, is_train=True, reuse=False):\n    \"\"\" 64x64 + (txt) --> real/fake \"\"\"\n    # https://github.com/hanzhanggit/StackGAN/blob/master/stageI/model.py\n    # Discriminator with ResNet : line 197 https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init=tf.random_normal_initializer(1., 0.02)\n    df_dim = 64  # 64 for flower, 196 for MSCOCO\n    s = 64 # output image size [64]\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\n    with tf.variable_scope(\"discriminator\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n        net_in = InputLayer(input_images, name='d_input/images')\n        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n                padding='SAME', W_init=w_init, name='d_h0/conv2d')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm')\n        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')\n        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm')\n\n        net = Conv2d(net_h3, df_dim*2, (1, 1), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='d_h4_res/conv2d')\n        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm')\n        net = Conv2d(net, df_dim*2, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d2')\n        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm2')\n        net = Conv2d(net, df_dim*8, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d3')\n        net = BatchNormLayer(net, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm3')\n        net_h4 = ElementwiseLayer(layer=[net_h3, net], combine_fn=tf.add, name='d_h4/add')\n        net_h4.outputs = tl.act.lrelu(net_h4.outputs, 0.2)\n\n        if t_txt is not None:\n            net_txt = InputLayer(t_txt, name='d_input_txt')\n            net_txt = DenseLayer(net_txt, n_units=t_dim,\n                   act=lambda x: tl.act.lrelu(x, 0.2),\n                   W_init=w_init, name='d_reduce_txt/dense')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim1')\n            net_txt = ExpandDimsLayer(net_txt, 1, name='d_txt/expanddim2')\n            net_txt = TileLayer(net_txt, [1, 4, 4, 1], name='d_txt/tile')\n            net_h4_concat = ConcatLayer([net_h4, net_txt], concat_dim=3, name='d_h3_concat')\n            # 243 (ndf*8 + 128 or 256) x 4 x 4\n            net_h4 = Conv2d(net_h4_concat, df_dim*8, (1, 1), (1, 1),\n                    padding='VALID', W_init=w_init, b_init=None, name='d_h3/conv2d_2')\n            net_h4 = BatchNormLayer(net_h4, act=lambda x: tl.act.lrelu(x, 0.2),\n                    is_train=is_train, gamma_init=gamma_init, name='d_h3/batch_norm_2')\n\n        net_ho = Conv2d(net_h4, 1, (s16, s16), (s16, s16), padding='VALID', W_init=w_init, name='d_ho/conv2d')\n        # 1 x 1 x 1\n        # net_ho = FlattenLayer(net_h4, name='d_ho/flatten')\n        logits = net_ho.outputs\n        net_ho.outputs = tf.nn.sigmoid(net_ho.outputs)\n    return net_ho, logits\n\ndef z_encoder(input_images, is_train=True, reuse=False):\n    \"\"\" 64x64 -> z \"\"\"\n    w_init = tf.random_normal_initializer(stddev=0.02)\n    gamma_init=tf.random_normal_initializer(1., 0.02)\n    df_dim = 64  # 64 for flower, 196 for MSCOCO\n    s = 64 # output image size [64]\n    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\n    with tf.variable_scope(\"z_encoder\", reuse=reuse):\n        tl.layers.set_name_reuse(reuse)\n\n        net_in = InputLayer(input_images, name='d_input/images')\n        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n                padding='SAME', W_init=w_init, name='d_h0/conv2d')\n\n        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h1/conv2d')\n        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm')\n        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h2/conv2d')\n        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')\n        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h3/conv2d')\n        net_h3 = BatchNormLayer(net_h3, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm')\n\n        net = Conv2d(net_h3, df_dim*2, (1, 1), (1, 1), act=None,\n                padding='VALID', W_init=w_init, b_init=None, name='d_h4_res/conv2d')\n        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm')\n        net = Conv2d(net, df_dim*2, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d2')\n        net = BatchNormLayer(net, act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm2')\n        net = Conv2d(net, df_dim*8, (3, 3), (1, 1), act=None,\n                padding='SAME', W_init=w_init, b_init=None, name='d_h4_res/conv2d3')\n        net = BatchNormLayer(net, #act=lambda x: tl.act.lrelu(x, 0.2),\n                is_train=is_train, gamma_init=gamma_init, name='d_h4_res/batchnorm3')\n        net_h4 = ElementwiseLayer(layer=[net_h3, net], combine_fn=tf.add, name='d_h4/add')\n        net_h4.outputs = tl.act.lrelu(net_h4.outputs, 0.2)\n\n        net_ho = FlattenLayer(net_h4, name='d_ho/flatten')\n        net_ho = DenseLayer(net_ho, n_units=z_dim, act=tf.identity,\n                W_init = w_init, name='d_ho/dense')\n\n        # w_init = tf.random_normal_initializer(stddev=0.02)\n        # b_init = None\n        # gamma_init = tf.random_normal_initializer(1., 0.02)\n        #\n        # net_in = InputLayer(input_images, name='p/in')\n        # net_h0 = Conv2d(net_in, df_dim, (5, 5), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),\n        #         padding='SAME', W_init=w_init, name='p/h0/conv2d')\n        #\n        # net_h1 = Conv2d(net_h0, df_dim*2, (5, 5), (2, 2), act=None,\n        #         padding='SAME', W_init=w_init, b_init=b_init, name='p/h1/conv2d')\n        # net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),\n        #         is_train=is_train, gamma_init=gamma_init, name='p/h1/batch_norm')\n        #\n        # net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,\n        #         padding='SAME', W_init=w_init, b_init=b_init, name='p/h2/conv2d')\n        # net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),\n        #         is_train=is_train, gamma_init=gamma_init, name='p/h2/batch_norm')\n        #\n        # net_h3 = Conv2d(net_h2, df_dim*8, (5, 5), (2, 2), act=None,\n        #         padding='SAME', W_init=w_init, b_init=b_init, name='p/h3/conv2d')\n        # net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),\n        #         is_train=is_train, gamma_init=gamma_init, name='p/h3/batch_norm')\n        #\n        # net_h4 = FlattenLayer(net_h3, name='p/h4/flatten')\n        # net_ho = DenseLayer(net_h4, n_units=z_dim,\n        #         act=tf.identity,\n        #         # act=tf.nn.tanh,\n        #         W_init = w_init, name='p/h4/output_real_fake')\n    return net_ho","metadata":{"_uuid":"a2df5164-44a4-4654-8001-eeaba9bd8ef1","_cell_guid":"864a4be8-2c14-41ff-b2f7-0839511e8c5f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-01-19T07:34:55.363549Z","iopub.execute_input":"2023-01-19T07:34:55.363947Z","iopub.status.idle":"2023-01-19T07:34:57.311605Z","shell.execute_reply.started":"2023-01-19T07:34:55.363918Z","shell.execute_reply":"2023-01-19T07:34:57.310405Z"},"trusted":true},"execution_count":null,"outputs":[]}]}