{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unzip(src_dir,new_name = None):\n\t# extract to current directory\n\tdirpath = '.'\n\ttry:\n\t\tif src_dir.endswith('.zip'):\n\t\t\tprint('unzipping ' + src_dir)\n\t\t\twith zipfile.ZipFile(src_dir) as zf:\n\t\t\t\tzip_dir = zf.namelist()[0]\n\t\t\t\tzf.extractall(dirpath)\n\t\telif src_dir.endswith('.tgz') or src_dir.endswith('tar.gz'):\n\t\t\tprint('unzipping ' + src_dir)\n\t\t\ttar = tarfile.open(src_dir)\n\t\t\ttar.extractall()\n\t\t\ttar.close()\n\t\t# os.remove(save_path)\n\t\tif new_name is None:\n\t\t\tpass\n\t\telse:\n\t\t\tos.rename('jpg', os.path.join(dirpath, new_name))\n\texcept:\n\t\traise('wrong format')","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:09:03.621352Z","iopub.execute_input":"2023-01-19T03:09:03.621790Z","iopub.status.idle":"2023-01-19T03:09:03.629992Z","shell.execute_reply.started":"2023-01-19T03:09:03.621750Z","shell.execute_reply":"2023-01-19T03:09:03.628822Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import tarfile\nimport os\nimage_dir=\"/kaggle/input/flowers-text-to-image/102flowers.tgz\"\nunzip(image_dir,'102flowers')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:09:07.740396Z","iopub.execute_input":"2023-01-19T03:09:07.740771Z","iopub.status.idle":"2023-01-19T03:09:14.674668Z","shell.execute_reply.started":"2023-01-19T03:09:07.740739Z","shell.execute_reply":"2023-01-19T03:09:14.673695Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"unzipping /kaggle/input/flowers-text-to-image/102flowers.tgz\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_folder_list(path=\"\"):\n    \"\"\"Return a folder list in a folder by given a folder path.\n    Parameters\n    ----------\n    path : a string or None\n        A folder path.\n    \"\"\"\n    return [os.path.join(path,o) for o in os.listdir(path) if os.path.isdir(os.path.join(path,o))]","metadata":{"execution":{"iopub.status.busy":"2023-01-19T02:23:03.771202Z","iopub.execute_input":"2023-01-19T02:23:03.771584Z","iopub.status.idle":"2023-01-19T02:23:03.777688Z","shell.execute_reply.started":"2023-01-19T02:23:03.771551Z","shell.execute_reply":"2023-01-19T02:23:03.776588Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom contextlib import contextmanager\n@contextmanager\ndef suppress_stdout():\n    \"\"\"Temporarily disable console output.\n    Examples\n    ---------\n    >>> print(\"You can see me\")\n    >>> with tl.ops.suppress_stdout():\n    >>>     print(\"You can't see me\")\n    >>> print(\"You can see me\")\n    References\n    -----------\n    - `stackoverflow <http://stackoverflow.com/questions/2125702/how-to-suppress-console-output-in-python>`_\n    \"\"\"\n    with open(os.devnull, \"w\") as devnull:\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\n","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:09:45.121487Z","iopub.execute_input":"2023-01-19T03:09:45.121880Z","iopub.status.idle":"2023-01-19T03:09:45.128621Z","shell.execute_reply.started":"2023-01-19T03:09:45.121850Z","shell.execute_reply":"2023-01-19T03:09:45.127586Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pip install tensorlayer","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:10:12.186875Z","iopub.execute_input":"2023-01-19T03:10:12.187299Z","iopub.status.idle":"2023-01-19T03:10:24.805172Z","shell.execute_reply.started":"2023-01-19T03:10:12.187268Z","shell.execute_reply":"2023-01-19T03:10:24.803934Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting tensorlayer\n  Downloading tensorlayer-2.2.5-py3-none-any.whl (381 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.2/381.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: imageio>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (2.19.3)\nRequirement already satisfied: requests>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (2.28.1)\nRequirement already satisfied: h5py>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (3.7.0)\nRequirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (1.7.3)\nRequirement already satisfied: cloudpickle>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (2.1.0)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (1.0.2)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (1.12.1)\nRequirement already satisfied: progressbar2>=3.39.3 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (4.2.0)\nRequirement already satisfied: scikit-image>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorlayer) (0.19.3)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio>=2.5.0->tensorlayer) (9.1.1)\nRequirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from progressbar2>=3.39.3->tensorlayer) (3.4.5)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.21.0->tensorlayer) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.21.0->tensorlayer) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.21.0->tensorlayer) (1.26.13)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.21.0->tensorlayer) (2022.12.7)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.15.0->tensorlayer) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.15.0->tensorlayer) (1.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.15.0->tensorlayer) (22.0)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.15.0->tensorlayer) (2.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.0->tensorlayer) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.0->tensorlayer) (1.0.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.15.0->tensorlayer) (5.1.1)\nInstalling collected packages: tensorlayer\nSuccessfully installed tensorlayer-2.2.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install scipy==1.2.2","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:19:13.202711Z","iopub.execute_input":"2023-01-19T03:19:13.203088Z","iopub.status.idle":"2023-01-19T03:19:29.891127Z","shell.execute_reply.started":"2023-01-19T03:19:13.203057Z","shell.execute_reply":"2023-01-19T03:19:29.889651Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting scipy==1.2.2\n  Downloading scipy-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (24.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scipy==1.2.2) (1.21.6)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.7.3\n    Uninstalling scipy-1.7.3:\n      Successfully uninstalled scipy-1.7.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwoodwork 0.16.4 requires scipy>=1.4.0, but you have scipy 1.2.2 which is incompatible.\ntpot 0.11.7 requires scipy>=1.3.1, but you have scipy 1.2.2 which is incompatible.\nstumpy 1.11.1 requires scipy>=1.5, but you have scipy 1.2.2 which is incompatible.\nstatsmodels 0.13.2 requires scipy>=1.3, but you have scipy 1.2.2 which is incompatible.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, but you have scipy 1.2.2 which is incompatible.\nscikit-image 0.19.3 requires scipy>=1.4.1, but you have scipy 1.2.2 which is incompatible.\nquantecon 0.6.0 requires scipy>=1.5.0, but you have scipy 1.2.2 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.2.2 which is incompatible.\nplotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.2.2 which is incompatible.\nphik 0.12.2 requires scipy>=1.5.2, but you have scipy 1.2.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\npandas-profiling 3.1.0 requires scipy>=1.4.1, but you have scipy 1.2.2 which is incompatible.\noptuna 3.0.5 requires scipy<1.9.0,>=1.7.0, but you have scipy 1.2.2 which is incompatible.\nnnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\nnilearn 0.9.2 requires scipy>=1.5, but you have scipy 1.2.2 which is incompatible.\nmatrixprofile 1.1.10 requires scipy<2.0.0,>=1.3.2, but you have scipy 1.2.2 which is incompatible.\njaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.2.2 which is incompatible.\njax 0.3.25 requires scipy>=1.5, but you have scipy 1.2.2 which is incompatible.\nimbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.0.1 which is incompatible.\nimbalanced-learn 0.10.1 requires scipy>=1.3.2, but you have scipy 1.2.2 which is incompatible.\ngiddy 2.3.3 requires scipy>=1.3.0, but you have scipy 1.2.2 which is incompatible.\nfeaturetools 1.11.1 requires scipy>=1.3.3, but you have scipy 1.2.2 which is incompatible.\nallennlp 2.10.1 requires scipy>=1.7.3, but you have scipy 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"help('scipy.misc.imresize')","metadata":{"execution":{"iopub.status.busy":"2023-01-19T03:21:44.860818Z","iopub.execute_input":"2023-01-19T03:21:44.861186Z","iopub.status.idle":"2023-01-19T03:21:44.867397Z","shell.execute_reply.started":"2023-01-19T03:21:44.861155Z","shell.execute_reply":"2023-01-19T03:21:44.866317Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"No Python documentation found for 'scipy.misc.imresize'.\nUse help() to get the interactive help utility.\nUse help(str) for help on the str class.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport nltk\nimport re\nimport string\nimport imageio\nimport skimage\nimport tensorlayer as tl\nfrom utils import *\n\n\ndataset = '102flowers' #\nneed_256 = True # set to True for stackGAN\n\n\n\nif dataset == '102flowers':\n    \"\"\"\n    images.shape = [8000, 64, 64, 3]\n    captions_ids = [80000, any]\n    \"\"\"\n    cwd = os.getcwd()\n    img_dir = os.path.join(cwd, '102flowers')\n    caption_dir = os.path.join(cwd, '/kaggle/input/flowers-dataset/text_c10')\n    VOC_FIR = cwd + '/vocab.txt'\n\n    ## load captions\n    caption_sub_dir = load_folder_list( caption_dir )\n    captions_dict = {}\n    processed_capts = []\n    for sub_dir in caption_sub_dir: # get caption file list\n        with suppress_stdout():\n            files = tl.files.load_file_list(path=sub_dir, regx='^image_[0-9]+\\.txt')\n            for i, f in enumerate(files):\n                file_dir = os.path.join(sub_dir, f)\n                key = int(re.findall('\\d+', f)[0])\n                t = open(file_dir,'r')\n                lines = []\n                for line in t:\n                    line = preprocess_caption(line)\n                    lines.append(line)\n                    processed_capts.append(tl.nlp.process_sentence(line, start_word=\"<S>\", end_word=\"</S>\"))\n                assert len(lines) == 10, \"Every flower image have 10 captions\"\n                captions_dict[key] = lines\n    print(\" * %d x %d captions found \" % (len(captions_dict), len(lines)))\n\n    ## build vocab\n    if not os.path.isfile('vocab.txt'):\n        _ = tl.nlp.create_vocab(processed_capts, word_counts_output_file=VOC_FIR, min_word_count=1)\n    else:\n        print(\"WARNING: vocab.txt already exists\")\n    vocab = tl.nlp.Vocabulary(VOC_FIR, start_word=\"<S>\", end_word=\"</S>\", unk_word=\"<UNK>\")\n\n    ## store all captions ids in list\n    captions_ids = []\n    try: # python3\n        tmp = captions_dict.items()\n    except: # python3\n        tmp = captions_dict.iteritems()\n    for key, value in tmp:\n        for v in value:\n            captions_ids.append( [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(v)] + [vocab.end_id])  # add END_ID\n            # print(v)              # prominent purple stigma,petals are white inc olor\n            # print(captions_ids)   # [[152, 19, 33, 15, 3, 8, 14, 719, 723]]\n            # exit()\n    captions_ids = np.asarray(captions_ids)\n    print(\" * tokenized %d captions\" % len(captions_ids))\n\n    ## check\n    img_capt = captions_dict[1][1]\n    print(\"img_capt: %s\" % img_capt)\n    print(\"nltk.tokenize.word_tokenize(img_capt): %s\" % nltk.tokenize.word_tokenize(img_capt))\n    img_capt_ids = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(img_capt)]#img_capt.split(' ')]\n    print(\"img_capt_ids: %s\" % img_capt_ids)\n    print(\"id_to_word: %s\" % [vocab.id_to_word(id) for id in img_capt_ids])\n\n    ## load images\n    with suppress_stdout():  # get image files list\n        imgs_title_list = sorted(tl.files.load_file_list(path=img_dir, regx='^image_[0-9]+\\.jpg'))\n    print(\" * %d images found, start loading and resizing ...\" % len(imgs_title_list))\n    s = time.time()\n\n    # time.sleep(10)\n    # def get_resize_image(name):   # fail\n    #         img = scipy.misc.imread( os.path.join(img_dir, name) )\n    #         img = tl.prepro.imresize(img, size=[64, 64])    # (64, 64, 3)\n    #         img = img.astype(np.float32)\n    #         return img\n    # images = tl.prepro.threading_data(imgs_title_list, fn=get_resize_image)\n    images = []\n    images_256 = []\n    for name in imgs_title_list:\n        # print(name)\n        img_raw = imageio.imread( os.path.join(img_dir, name) )\n        # (64, 64, 3)\n        img=skimage.transform.resize(img_raw,[64,64])\n        img = img.astype(np.float32)\n        images.append(img)\n        if need_256:\n            img = skimage.transform.resize(img_raw,[256, 256]) # (256, 256, 3)\n            img = img.astype(np.float32)\n\n            images_256.append(img)\n    # images = np.array(images)\n    # images_256 = np.array(images_256)\n    print(\" * loading and resizing took %ss\" % (time.time()-s))\n\n    n_images = len(captions_dict)\n    n_captions = len(captions_ids)\n    n_captions_per_image = len(lines) # 10\n\n    print(\"n_captions: %d n_images: %d n_captions_per_image: %d\" % (n_captions, n_images, n_captions_per_image))\n\n    captions_ids_train, captions_ids_test = captions_ids[: 8000*n_captions_per_image], captions_ids[8000*n_captions_per_image :]\n    images_train, images_test = images[:8000], images[8000:]\n    if need_256:\n        images_train_256, images_test_256 = images_256[:8000], images_256[8000:]\n    n_images_train = len(images_train)\n    n_images_test = len(images_test)\n    n_captions_train = len(captions_ids_train)\n    n_captions_test = len(captions_ids_test)\n    print(\"n_images_train:%d n_captions_train:%d\" % (n_images_train, n_captions_train))\n    print(\"n_images_test:%d  n_captions_test:%d\" % (n_images_test, n_captions_test))\n\n    ## check test image\n    # idexs = get_random_int(min=0, max=n_captions_test-1, number=64)\n    # temp_test_capt = captions_ids_test[idexs]\n    # for idx, ids in enumerate(temp_test_capt):\n    #     print(\"%d %s\" % (idx, [vocab.id_to_word(id) for id in ids]))\n    # temp_test_img = images_train[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n    # save_images(temp_test_img, [8, 8], 'temp_test_img.png')\n    # exit()\n\n    # ## check the first example\n    # tl.visualize.frame(I=images[0], second=5, saveable=True, name='temp', cmap=None)\n    # for cap in captions_dict[1]:\n    #     print(cap)\n    # print(captions_ids[0:10])\n    # for ids in captions_ids[0:10]:\n    #     print([vocab.id_to_word(id) for id in ids])\n    # print_dict(captions_dict)\n\n    # ## generate a random batch\n    # batch_size = 64\n    # idexs = get_random_int(0, n_captions_test, batch_size)\n    # # idexs = [i for i in range(0,100)]\n    # print(idexs)\n    # b_seqs = captions_ids_test[idexs]\n    # b_images = images_test[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n    # print(\"before padding %s\" % b_seqs)\n    # b_seqs = tl.prepro.pad_sequences(b_seqs, padding='post')\n    # print(\"after padding %s\" % b_seqs)\n    # # print(input_images.shape)   # (64, 64, 64, 3)\n    # for ids in b_seqs:\n    #     print([vocab.id_to_word(id) for id in ids])\n    # print(np.max(b_images), np.min(b_images), b_images.shape)\n    # from utils import *\n    # save_images(b_images, [8, 8], 'temp2.png')\n    # # tl.visualize.images2d(b_images, second=5, saveable=True, name='temp2')\n    # exit()\n\nimport pickle\ndef save_all(targets, file):\n    with open(file, 'wb') as f:\n        pickle.dump(targets, f)\n\nsave_all(vocab, '_vocab.pickle')\nsave_all((images_train_256, images_train), '_image_train.pickle')\n#save_all((images_test_256, images_test), '_image_test.pickle')\nsave_all((n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test), '_n.pickle')\nsave_all((captions_ids_train, captions_ids_test), '_caption.pickle')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-01-19T03:36:10.783843Z","iopub.execute_input":"2023-01-19T03:36:10.784258Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" * 8189 x 10 captions found \nWARNING: vocab.txt already exists\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:95: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","output_type":"stream"},{"name":"stdout","text":" * tokenized 81890 captions\nimg_capt: this flower has bright purple  spiky petals  and greenish sepals below them \nnltk.tokenize.word_tokenize(img_capt): ['this', 'flower', 'has', 'bright', 'purple', 'spiky', 'petals', 'and', 'greenish', 'sepals', 'below', 'them']\nimg_capt_ids: [6, 3, 7, 31, 18, 165, 4, 5, 318, 83, 374, 120]\nid_to_word: ['this', 'flower', 'has', 'bright', 'purple', 'spiky', 'petals', 'and', 'greenish', 'sepals', 'below', 'them']\n * 8189 images found, start loading and resizing ...\n * loading and resizing took 659.7158839702606s\nn_captions: 81890 n_images: 8189 n_captions_per_image: 10\nn_images_train:8000 n_captions_train:80000\nn_images_test:189  n_captions_test:1890\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install utils","metadata":{"execution":{"iopub.status.busy":"2023-01-19T02:18:56.043766Z","iopub.execute_input":"2023-01-19T02:18:56.044163Z","iopub.status.idle":"2023-01-19T02:19:07.042178Z","shell.execute_reply.started":"2023-01-19T02:18:56.044132Z","shell.execute_reply":"2023-01-19T02:19:07.040623Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting utils\n  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\nInstalling collected packages: utils\nSuccessfully installed utils-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]}]}